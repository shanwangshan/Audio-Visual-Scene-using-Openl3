#+TITLE: TAU urban audio visual scenes 2021

** Getting started
*** Clone this repository

   #+BEGIN_SRC sh :eval no

   git clone https://github.com/shanwangshan/TAU-urban-audio-visual-scenes.git

   #+END_SRC
*** Download the features

    Here is the [[https://tuni-my.sharepoint.com/:u:/g/personal/shanshan_wang_tuni_fi/EWJJrSwAJOhEkkI0ozOGXmgBmGDfDvVosiere77aTVTIYg?e=tph1Pe][link]] to download features. After extracting the zip file, please put it under =./create_data/= folder.

*** Setup the conda environment

    #+BEGIN_SRC sh :eval no

      conda create --name <env> --file requirements_torch.txt
      conda create --name <env> --file requirements_openl3.txt

    #+END_SRC
*It is only needed to setup the openl3 environment if you want to create the features by yourself. Otherwise, only creating torch environment is enough*. This is because openl3 environment is used to create the embedding features using the pretrained L3 network. If you are interested in L3 embedding, we refer researchers to read  [[https://openl3.readthedocs.io/en/latest/tutorial.html#extracting-image-embeddings][OpenL3]].
*** Structure of this repository



** ICASSP paper results

   #+BEGIN_SRC sh :eval: no

     cd train
     python test.py --features_path '../create_data/features_data/' --model_type 'audio'
     python test.py --features_path '../create_data/features_data/' --model_type 'video'
     python test.py --features_path '../create_data/features_data/' --model_type 'audio_video'

   #+END_SRC
#+OPTIONS: ^:nil
if model_type is set to audio, which means audio sub-network is to be tested.
if model_type is set to video, which means video sub-network is to be tested.
if model_type is set to audio_video, which means the early fusion network is to be tested.
#+BEGIN_SRC sh :eval:no

  cd ../train_combine/
  python test.py --features_path '../create_data/features_data/' --model_audio_path '../train/audio_model/model.pt' --model_video_path '../train/video_model/model.pt'

#+END_SRC
This is to test the proposed method, which requires weights trained from audio subnetwork and video subnetwork.

*** Results

| Method                    | Acurracy |
|---------------------------+----------|
| Audio only                |    75.8% |
| Video only                |    68.4% |
| Early A-V fusion          |    82.2% |
| Proposed early A-V fusion |  *84.8%* |

** DCASE2021 Task1 Subtask B Baseline

   #+BEGIN_SRC sh :eval: no

     cd train
     python test_DCASE.py --features_path '../create_data/features_data/' --model_type 'audio'
     python test_DCASE.py --features_path '../create_data/features_data/' --model_type 'video'
     python test_DCASE.py --features_path '../create_data/features_data/' --model_type 'audio_video'

   #+END_SRC
#+OPTIONS: ^:nil
if model_type is set to audio, which means audio sub-network is to be tested.
if model_type is set to video, which means video sub-network is to be tested.
if model_type is set to audio_video, which means the early fusion network is to be tested.

#+BEGIN_SRC sh :eval:no

  cd ../train_combine/
  python test_DCASE.py --features_path '../create_data/features_data/' --model_audio_path '../train/audio_model/model.pt' --model_video_path '../train/video_model/model.pt'

#+END_SRC
This is to test the proposed method, which requires weights trained from audio subnetwork and video subnetwork.

*** Results

| Method                      | Logloss(Primary metrics) | Acurracy |
|-----------------------------+--------------------------+----------|
| Audio only                  |                    1.048 |    65.1% |
| Video only                  |                    1.648 |    64.9% |
| Early A-V fusion            |                    0.963 |    77.5% |
| *Proposed early A-V fusion* |                 * 0.658* |  *77.0%* |

*** Command for creating examples
    To help researchers understand the dataset more intuitively, under the dataset folder, we created 20 video examples where video frames are played together with its audio frames. The command to create these videos is,

   #+BEGIN_SRC .sh :eval:no

   ffmpeg -i <video filename> -i <audio filename> -shortest -strict -2 <output filename>
   #+END_SRC

** Citation

   If our work is useful to you then please cite us as:

  #+BEGIN_SRC

  @INPROCEEDINGS{wangicassap2021,
  title={A Curated Dataset of Urban Scenes for Audio-Visual Scene Analysis},
  booktitle={ICASSP 2021 - 2021 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)},
  author={Wang, Shanshan and Mesaros, Annamaria and Heittola, Toni and Virtanen, Tuomas}, year={2021}}

  #+END_SRC
